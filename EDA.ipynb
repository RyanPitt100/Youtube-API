{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba59ef57-dc09-43ac-ba08-6aae1a3ef871",
   "metadata": {},
   "source": [
    "\n",
    "# Exploratory Data Analysis of S.T.E.M Youtube Channels\n",
    "\n",
    "## 1. Aims, objectives and background\n",
    "\n",
    "### 1.1 Introduction\n",
    "\n",
    "By daily searches, YouTube is the second largest search engine on the internet behind Google. With over 122 million daily users and 62% of U.S. internet users using the platform daily[[1]](https://thesocialshepherd.com/blog/youtube-statistics#:~:text=YouTube%20has%202.1%20billion%20monthly,122%20million%20users%20per%20day), the reach and scope of content that can gain traction on the platform is among the largest in the world. This has made it one of the first sites that educators go to in order to share their content with users, and also one of the go-to places for students looking for educational content. However, the algorithm behind YouTube's video recommendations is often changing and not completely understood by most users despite being one of the largest scale recommendation systems in use today[[2]](https://dl.acm.org/doi/10.1145/2959100.2959190).  \n",
    "One of the main factors in the mainstream is that if a video receives more likes and comments (known as \"engagement) then it will be promoted to more users. Another big consideration to many youtubers is video duration. Short videos are often watched all the way through, while longer ones have more opportunities to place advertisments (which benefits YouTube so logically would be rewarded by YouTube's algorithm) but may find watchers losing interest and leaving the video halfway through.[[3]](https://vidiq.com/blog/post/5-youtube-algorithm-myths-youtubers-need-to-know-about/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa839bf-dfc1-4858-931a-3ab9db37ea87",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Aims and objectives\n",
    "\n",
    "Within this project, I will explore the following:\n",
    "\n",
    "- Understand the YouTube API and use it to obtain video/channel metadata.\n",
    "- Analyze this data to understand best practices among S.T.E.M. YouTube channels in order to grow a channel's audience.\n",
    "- Explore the trending topics within the S.T.E.M. YouTube community using NLP techniques.\n",
    "    - What topics are evergreen and a constant source of curiosity out of viewers? \n",
    "    - What questions are being asked in comments of these videos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca7bc1-6007-48ac-819d-87cd4543124e",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Steps of this project\n",
    "\n",
    "1. Obtain video metadata via the YouTube API of mainstream S.T.E.M. communicators on YouTube. \n",
    "    - This contains many smaller steps, including: creating a developer key, requesting data from the API and transforming this data into something usable.\n",
    "2. Preprocess and clean the data.\n",
    "3. Build additional features for analysis.\n",
    "4. Exploratory Data Analysis.\n",
    "5. Conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366dbac2-a70f-4402-9f89-5ee51543a4e6",
   "metadata": {},
   "source": [
    "\n",
    "### 1.4 The Dataset\n",
    "\n",
    "#### Data Selection\n",
    "\n",
    "As this project is focused on S.T.E.M. channels, there are not many available datasets that contain realtime data that is suitable for this purpose. As such, I have decided to use the [Google YouTube Data API version 3.0](https://developers.google.com/youtube/v3) to build my own dataset to explore. \n",
    "\n",
    "For analysis of a broader scope, there is a constantly updated dataset [available on Kaggle](https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset) containing the top 200 most trending videos of each day for the last several months. \n",
    "\n",
    "Complete steps for how I collected and built the dataset is contained in section *2. Data Creation*\n",
    "\n",
    "#### Data Limitations\n",
    "\n",
    "This dataset contains fully up-to-date, real-time and real-world data. However because this dataset was set up to look for channels that I am familiar with, the scope of the dataset might not reach as far as it could be with additional automation. Another consideration is what consistutes a 'popular' or 'large' channel. There are many metrics with which to measure a channel's popularity, I have mostly weighted the subscriber count as the main factor. However, there are many channels that recieve large viewcounts without gaining a large subscriber count because of the nature of the content. There are also channels that may focus on a more specific area within S.T.E.M. and therefore draw a smaller audience despite having a large audience for their niche. A final consideration is that without requesting additional request units from YouTube the API is limited to 10,000 requests per day. And so I have limited the scope of this EDA to remain within that limit. However, if desired a larger dataset could be generated by requesting different channels data on each day, though it would be slightly less up-to-date.\n",
    "\n",
    "#### Ethics of data source\n",
    "\n",
    "All data obtained in this dataset was done so through YouTube's API, the usage of which is free of charge given the the application send requests are within a certain limit. \"The YouTube API uses a quota to ensure that developers use the service as intended and do not create applications that unfairly reduce service quality or limit access for others.\" Additional unit allocations can be requests via the YouTube API site but that is unneccesary for a project of this scope. \n",
    "\n",
    "The data in this project is also public, and could be collected manually just by looking at each channel's page in your internet browser. No information that has been collected is hidden or private. Additionally, the data is used only for research purposes and not for any commercial ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48fe76-0cb0-47fc-9c60-f84d90d28b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import isodate \n",
    "from dateutil import parser\n",
    "# Data viz packages\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85317f28-baa4-4ceb-83a0-08790e0b3e77",
   "metadata": {},
   "source": [
    "## 2. Data creation with Youtube API\n",
    "\n",
    "To begin this project, I used my own personal google account to create a project on Google Developers Console. Next, I created an API key. Next I searched for the target YouTube channels to be analysed and collected the channel ID of each one. Finally I enabled the API for use in this application and imported the channel IDs into an array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7469c10-cb7e-4aaf-a0c8-a30b62a892d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyBe804qZLpGzADIOi0yUXaO4qcwDJiSo3w'\n",
    "\n",
    "channel_ids = [#'UCoxcjq-8xIDTYp3uz647V5A', #numberphile\n",
    "               #'UC9-y-6csu5WGm29I7JiwpnA', #computerphile\n",
    "               'UCYO_jab_esuFRV4b17AJtAw', #1blue3brown\n",
    "               #'UCbfYPyITQ-7l4upoX8nvctg', #2MinutePapers\n",
    "               \n",
    "              ]\n",
    "               \n",
    "#Setting up api connection. \n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = 'v3'\n",
    "\n",
    "#Get credentials and create API client.\n",
    "youtube = build(api_service_name, api_version, developerKey = api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fdc538-fb5e-48f7-84df-233b8010ebf5",
   "metadata": {},
   "source": [
    "Following this, I wrote some methods that would allow requests to be sent to YouTube which collects various data from these channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ed2a33f-9640-4252-b9c2-c7965c0e0885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube, channel_ids):\n",
    "    '''\n",
    "    Get channel stats\n",
    "    Params:\n",
    "    ----\n",
    "    youtube: build object from youtube API\n",
    "    channel_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    -----\n",
    "    dataframe of all channel stats for each id in the channel_ids list.\n",
    "    '''\n",
    "    \n",
    "    all_data = []\n",
    "    request = youtube.channels().list(part = 'snippet,contentDetails,statistics', \n",
    "                                        id = ', '.join(channel_ids))\n",
    "    response = request.execute()\n",
    "\n",
    "    #loop to store data in dictionary.\n",
    "    for item in response['items']:\n",
    "        data = {'channelName' : item['snippet']['title'],\n",
    "                'subscribers' : item['statistics']['subscriberCount'],\n",
    "                'views' : item['statistics']['viewCount'],\n",
    "                'totalVideos' : item['statistics']['videoCount'],\n",
    "                'playlistId' : item['contentDetails']['relatedPlaylists']['uploads']\n",
    "               }\n",
    "        all_data.append(data)\n",
    "\n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fc52b57-e9f0-43f4-b7d3-b13c91e1ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids(youtube, playlist_ids):\n",
    "    '''\n",
    "    Get video ids for channels in channel_ids\n",
    "    Params:\n",
    "    ----\n",
    "    youtube: build object from youtube API\n",
    "    playlist_ids: list of channels' playlist IDs\n",
    "    \n",
    "    Returns:\n",
    "    -----\n",
    "    Array of id values for each video in each channels \n",
    "    '''\n",
    "    \n",
    "    video_ids = []\n",
    "    \n",
    "    request = youtube.playlistItems().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            playlistId=playlist_id,\n",
    "            maxResults = 50\n",
    "        )\n",
    "    response = request.execute()\n",
    "    \n",
    "    #loop to collect all ids.\n",
    "    for item in response['items']:\n",
    "        video_ids.append(item['contentDetails']['videoId'])\n",
    "        \n",
    "        \n",
    "    next_page_token = response.get('nextPageToken')\n",
    "\n",
    "    while next_page_token is not None:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet, contentDetails\",\n",
    "            playlistId=playlist_id,\n",
    "            maxResults = 50\n",
    "        )\n",
    "        response = request.execute()\n",
    "        #loop to collect all ids.\n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['contentDetails']['videoId'])\n",
    "\n",
    "\n",
    "        next_page_token = response.get('nextPageToken') \n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bccc017-29d2-424a-8bb4-064b0cbd5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(youtube, video_ids):\n",
    "    '''\n",
    "    Get video statistics for all videos contained in each playlist of playlist_ids\n",
    "    Params:\n",
    "    ----\n",
    "    youtube: build object from youtube API\n",
    "    playlist_id: list of channels' playlist IDs\n",
    "    \n",
    "    Returns:\n",
    "    -----\n",
    "    Dataframe of video ids with various statistics such at Video Title, date of upload, viewcount etc.\n",
    "    '''\n",
    "    all_video_info = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet, contentDetails, statistics\",\n",
    "            id = ','.join(video_ids[i: i+50])\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "\n",
    "        for video in response['items']:\n",
    "            stats_to_keep = {'snippet' : ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n",
    "                             'statistics' : ['viewCount', 'likeCount', 'commentCount'],\n",
    "                             'contentDetails' : ['duration', 'definition', 'caption']\n",
    "                            }\n",
    "            video_info = {}\n",
    "            video_info['video_id'] = video['id']\n",
    "\n",
    "            for k in stats_to_keep.keys():\n",
    "                for v in stats_to_keep[k]:\n",
    "                    try:\n",
    "                        video_info[v] = video[k][v]\n",
    "                    except: \n",
    "                        video_info[v] = None\n",
    "\n",
    "            all_video_info.append(video_info)\n",
    "    return pd.DataFrame(all_video_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bd0be74-296c-4774-9d6a-3829af40616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_in_videos(youtube, video_ids):\n",
    "    '''\n",
    "    Get top level comments as text from all videos given in video_ids (max 10 comments due to API's request quota).\n",
    "    Params:\n",
    "    youtube: the build object from googleapliclient.discovery\n",
    "    video_ids: the array of video ids\n",
    "    \n",
    "    Returns: \n",
    "    DataFrame with video IDs and top level comments of each video.\n",
    "    '''\n",
    "    all_comments = []\n",
    "    \n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part = \"snippet,replies\",\n",
    "                videoId = video_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]]\n",
    "            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n",
    "\n",
    "            all_comments.append(comments_in_video_info)\n",
    "            \n",
    "        except:\n",
    "            #When error occurs because of commends being disabled on a video etc.\n",
    "            print(f\"Could not collect comments for video {video_id}\")\n",
    "        \n",
    "    return pd.DataFrame(all_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab8cf2-0369-499a-854a-e6263e8c4639",
   "metadata": {},
   "source": [
    "\n",
    "## Get overall channel statistics\n",
    "\n",
    "Using the first defined method above, get_channel_stats, we are going to obtain the channel stats for each channel in our array of channel_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "204f0cbf-54bf-417a-afec-49615cac5b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/channels?part=snippet%2CcontentDetails%2Cstatistics&id=UCYO_jab_esuFRV4b17AJtAw&key=AIzaSyBe804qZLpGzADIOi0yUXaO4qcwDJiSo3w&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m channel_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_channel_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43myoutube\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m channel_data\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mget_channel_stats\u001b[1;34m(youtube, channel_ids)\u001b[0m\n\u001b[0;32m     14\u001b[0m all_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39mchannels()\u001b[38;5;241m.\u001b[39mlist(part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet,contentDetails,statistics\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     16\u001b[0m                                     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(channel_ids))\n\u001b[1;32m---> 17\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#loop to store data in dictionary.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/channels?part=snippet%2CcontentDetails%2Cstatistics&id=UCYO_jab_esuFRV4b17AJtAw&key=AIzaSyBe804qZLpGzADIOi0yUXaO4qcwDJiSo3w&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "channel_data = get_channel_stats(youtube, channel_ids)\n",
    "\n",
    "channel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eefe3d-fa57-493c-9501-a4131e7d383e",
   "metadata": {},
   "source": [
    "Looking at the datatypes for the columns in this dataframe, many of the 'count' columns are currently stored as objects. This should be changed to more easily access this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234caa7b-329f-4103-84ea-21d7527f69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert count columns to numeric columns.\n",
    "numeric_cols = ['subscribers', 'views', 'totalVideos']\n",
    "channel_data[numeric_cols] = channel_data[numeric_cols].apply(pd.to_numeric, errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ec104-c03f-42e7-aef9-45b6a6b93478",
   "metadata": {},
   "source": [
    "With this fixed, we can look at the number of subscribers for each channel to understand the relative popularity of these channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7adb4-8b73-454a-9e90-d2c864e86ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (10, 8)})\n",
    "ax = sns.barplot(data = channel_data.sort_values('subscribers', \n",
    "                ascending = False), x = 'channelName', y = 'subscribers')\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\n",
    "plot = ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e7551-88a3-4c97-9328-9c2184579b7e",
   "metadata": {},
   "source": [
    "Next, a look at the rankings based on total views across all videos rather than number of subscribers as overall popularity can be based on several of these metrics not only one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91586f7-dd3e-4081-9971-38711b23fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data = channel_data.sort_values('views', ascending = False), x = 'channelName', y = 'views')\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\n",
    "plot = ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4917a5-a143-4d16-b581-3a496dddf6df",
   "metadata": {},
   "source": [
    "## Get video statistics for all channels\n",
    "\n",
    "Now, we will gather data from all videos among all channels in our channel_ids array. In total we collected metadata for 2322 videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73f718-e6c5-44ae-9471-3c4290f5510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with video statistics and comments from all channels\n",
    "\n",
    "video_df = pd.DataFrame()\n",
    "comments_df = pd.DataFrame()\n",
    "\n",
    "for c in channel_data['channelName'].unique():\n",
    "    print(\"Getting video information from channel: \" + c)\n",
    "    playlist_id = channel_data.loc[channel_data['channelName']== c, 'playlistId'].iloc[0]\n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "    \n",
    "    # get video data\n",
    "    video_data = get_video_details(youtube, video_ids)\n",
    "    # get comment data\n",
    "    comments_data = get_comments_in_videos(youtube, video_ids)\n",
    "\n",
    "    # append video data together and comment data toghether\n",
    "    video_df = video_df.append(video_data, ignore_index=True)\n",
    "    comments_df = comments_df.append(comments_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ce621cb-0c40-4139-83ce-22d6e68fb4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca28a6a",
   "metadata": {},
   "source": [
    "For comment_df we limited the number of comments we recieved in order to not go over the daily 10,000 unit request quota that the API has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe4f866d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comment_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcomment_df\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comment_df' is not defined"
     ]
    }
   ],
   "source": [
    "comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv file.\n",
    "video_df.to_csv(\"video_data_youtube_eda.csv\")\n",
    "comment_df.to_csv(\"comment_data_youtube_eda.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe9e7d5",
   "metadata": {},
   "source": [
    "## Preprocessing & Feature engineering\n",
    "\n",
    "In order to effectively analyze the data, a few pre-processing steps have to be taken. Firstly, some columns should be reformatted, particularly the publishedAt and duration columns which are not in a logical format currently. Also, enriching the data with some new features that combine or refactor some of the data currently collected would help as well.\n",
    "\n",
    "### Check for empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee8468",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.publishedAt.sort_values().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d87fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['viewCount', 'likeCount', 'commentCount']\n",
    "video_df[cols] = video_df[cols].apply(pd.to_numeric, errors = 'coerce', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1d1f8",
   "metadata": {},
   "source": [
    "### Enriching data\n",
    "\n",
    "Some data can be enriched to help with analysis, for example:\n",
    "\n",
    "- creating a column that shows on which day of the week the video was uploaded.\n",
    "- convert duration column into seconds.\n",
    "- calculate comments and likes per 1000 views.\n",
    "- calculate title character length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e85839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish data column\n",
    "video_df['publishedAt'] = video_df['publishedAt'].apply(lambda x: parser.parse(x))\n",
    "video_df['publishedDay'] = video_df['publishedAt'].apply(lambda x: x.strftime(\"%A\"))\n",
    "\n",
    "#convert duration to seconds\n",
    "video_df['durationSecs'] = video_df['duration'].apply(lambda x: isodate.parse_duration(x))\n",
    "video_df['durationSecs'] = video_df['durationSecs'].astype('timedelta64[s]')\n",
    "\n",
    "#add number of tags for each video as a column\n",
    "video_df['tagsCount'] = video_df['tags'].apply(lambda x: 0 if x is None else len(x))\n",
    "\n",
    "#comments and likes per 1000 views\n",
    "video_df['likeRatio'] = video_df['likeCount'] / video_df['viewCount'] * 1000\n",
    "video_df['commentRatio'] = video_df['commentCount'] / video_df['viewCount'] * 1000\n",
    "\n",
    "# Title length\n",
    "video_df['titleLength'] = video_df['title'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4883f8ed",
   "metadata": {},
   "source": [
    "Check the data set to ensure everything has come out as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7df80",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "### Views distribution per channel\n",
    "\n",
    "We can see how the views are distributed across each channel in the dataset using the video statistics we have collected. Some channels will be shown to have a consistent views per video while others more spikey as they fluctuate between trending and not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da090d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18,6)\n",
    "sns.violinplot(video_df['channelTitle'], video_df['viewCount'])\n",
    "plt.title(\"Views per channel\", loc = 'right', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4984f7c",
   "metadata": {},
   "source": [
    "### Does having more comments and likes correlate with viewcount?\n",
    "It seems logical that more people watching a video will result in more likes and comments, but as some channels have more commited repeat viewers than others, does this actually hold true in reality? \n",
    "Below shows a plot of this and suggests there is infact a strong correlation. However, likes and comments have a stronger correlation to each other than to the viewcount. As well as this, the number of likes seems to correlate more heavily than the number of comments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "sns.scatterplot(data = video_df, x = 'commentCount', y = 'viewCount', ax = ax[0])\n",
    "sns.scatterplot(data = video_df, x = 'likeCount', y = 'viewCount', ax = ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12657259",
   "metadata": {},
   "source": [
    "Next, looking at the ratio of likes/comments to 1000 views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplot(1,2)\n",
    "sns.scatterplot(data = video_df, x = 'commentRatio', y = 'viewCount', ax = ax[0])\n",
    "sns.scatterplot(data = video_df, x = 'likeRatio', y = 'viewCount', ax = ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0996f",
   "metadata": {},
   "source": [
    "After comparing these figures with the absolute number of comments and likes, the correlation is much less clear. The connection between comments and view counts seems to diminish. This would suggest that channels with a high comment-view ratio have built a close community of viewers, who are willing to discuss the video with each other instead of moving onto another once they have finished watching. \n",
    "\n",
    "### Does video duration matter for views and interactions?\n",
    "\n",
    "Many YouTube experts claim that the duration of a video is important for generating a higher 'weight' in the YouTube algorithm for a video. And that videos under or over a certain limit won't be pushed as much by the recommendation engine[[4]](https://piktochart.com/blog/ideal-video-length/#:~:text=Ideal%20YouTube%20video%20length%3A%205,lasting%205%20to%2015%20minutes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113579e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = video_df[video_df['durationSecs'] < 10000], x = 'durationSecs', bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284de48",
   "metadata": {},
   "source": [
    "Next a look at duration against comment and like count. It appears that ______ shorter/longer videos tend to get more likes and comments than longer/shorter ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b6994",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "sns.scatterplot(data = video_df, x = \"durationSecs\", y = \"commentCount\", ax = ax[0])\n",
    "sns.scatterplot(data = video_df, x = \"durationSecs\", y = \"likeCount\", ax = ax[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6925f79",
   "metadata": {},
   "source": [
    "### Is title length important for attracting viewers? \n",
    "\n",
    "This is another consideration often mentioned by experts claiming that title lengths between 60-70 characters[[5]](https://vidiq.com/blog/post/youtube-video-title/#:~:text=Stay%20Under%20the%20YouTube%20Title%20Character%20Limit&text=On%20YouTube%2C%20you%20have%20a,up%20as%20a%20truncated%20title) will encourage the algorithm to recommend that particular video. But is there any weight to this claim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = video_df, x = \"titleLength\", y = 'viewCount')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e6a435",
   "metadata": {},
   "source": [
    "## Wordcloud for title buzzwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f237d04b",
   "metadata": {},
   "source": [
    "It is also interesting to see what common topics or words are used in titled in this demographic and which words most frequently appear. Below shows the wordcloud for the most common words in video titles among those in the dataset. However, stopwords such as \"you\" and \"I\" have not been counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "video_df['title_no_stopwords'] = video_df['title'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n",
    "\n",
    "all_words = list([a for b in video_df['title_no_stopwords'].tolist() for a in b])\n",
    "all_words_str = \" \".join(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize = (30, 30))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    \n",
    "wordcloud = WordCloud(width = 2000, height = 1000, random_state = 1, background_color = 'white', \n",
    "                     colormap = 'viridis', collocation = False).generate(all_words_str)\n",
    "\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadaa174",
   "metadata": {},
   "source": [
    "(TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbdfb8a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
